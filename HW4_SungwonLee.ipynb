{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRfi_A6L-ZN_"
      },
      "source": [
        "# HW4: Graph Neural Networks\n",
        "\n",
        "* There are two datasets in the `data` folder: `train.pt`, `test.pt`. You will train a GCNN on the train dataset, then make predictions on the test dataset.\n",
        "* There are two parts in this notebook. `Part I` gives a custom `Dataset` object and loads the datasets. The `QM_Dataset` object inherites from torch geometric `Dataset` object. `Part II` is an example solution.\n",
        "* This HW is implemented with [Pytorch Geometric (PyG)](https://pytorch-geometric.readthedocs.io/en/latest/index.html). Another popular library for implementing GNNs is [Deep Graph Library (DGL)](https://www.dgl.ai/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71pd8cli6mmR",
        "outputId": "2a26b002-f76c-490d-c242-ec9e476510fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.10/dist-packages (2.5.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.11.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2023.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.9.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6z5zXzc2-ZOD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear, ReLU, Sequential\n",
        "from torch_geometric.data import Dataset\n",
        "from torch_geometric.loader import DataLoader\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import torch_geometric.nn as pyg_nn\n",
        "from torch_geometric.nn import TransformerConv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlnhZwRa6iZP",
        "outputId": "504ea5fa-79ab-47dc-b806-562443db9395"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# only on google colab\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrdqqRyp-ZOE"
      },
      "source": [
        "## Part I. The training and testing dataset are provided\n",
        "\n",
        "- The train and test datasets were pre-processed graphs. The train dataset contains 20,000 graphs, while the test dataset contains 2,000 graphs.\n",
        "- Each graph contains the following components:\n",
        "\n",
        "    - `x`, the matrix containing node features, `[num_of_nodes, num_node_features=11]`\n",
        "    - `edge_index`, the matrix containing connection information about different nodes, `[2, num_of_edges]`\n",
        "    - `y`, the label for the graph, `scaler`. The value is set to `0` in the test dataset\n",
        "    - `pos`, the matrix containing the node positions, `[num_of_nodes, 3]`\n",
        "    - `edge_attr`, the matrix containing the edge information, `[num_edges, 4]`\n",
        "    - `names`, index for the graph. For example, `gdb_59377`\n",
        "\n",
        "- Depending on the graph convolutional layer that is used, different components are needed. For the most basic application, `x`, `edge_index` and `y` will be used.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4uVaHtpZ8nwV"
      },
      "outputs": [],
      "source": [
        "class QM_Dataset(Dataset):\n",
        "    def __init__(self, path):\n",
        "        super().__init__(root=\".\")\n",
        "        self.data = torch.load(path)\n",
        "\n",
        "    def len(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def get(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "train_path = \"/content/drive/MyDrive/Colab Notebooks/train_path\"\n",
        "test_path = \"/content/drive/MyDrive/Colab Notebooks/test_path\"\n",
        "\n",
        "train_data_ = QM_Dataset(train_path)\n",
        "\n",
        "# train dataset can be split for validation purposes\n",
        "train_data, validate_data = torch.utils.data.random_split(train_data_, [19000, 1000])\n",
        "test_data = QM_Dataset(test_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMYYRRR0-ZOG"
      },
      "source": [
        "## Part II. Example solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IMZVJsa88Y8p"
      },
      "outputs": [],
      "source": [
        "# define the network\n",
        "# many convolutional layers are available in torch_geometric.nn\n",
        "# here NNConv is just used as an example\n",
        "\n",
        "from torch_geometric.nn import NNConv, Set2Set, GCNConv, SGConv, TAGConv\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, num_features=11, dim=64):\n",
        "        super().__init__()\n",
        "        self.lin0 = torch.nn.Linear(num_features, dim)\n",
        "        nn = Sequential(Linear(4, 128), ReLU(), Linear(128, dim))\n",
        "        self.conv = TAGConv(dim, dim, K = 6)      # replace with your own convolutional layers here\n",
        "        self.set2set = Set2Set(dim, processing_steps=3)    # set2set is used to map from nodes to graphs\n",
        "        self.lin1 = torch.nn.Linear(2 * dim, dim)\n",
        "        self.lin2 = torch.nn.Linear(dim, 1)\n",
        "\n",
        "    def forward(self, data):\n",
        "        out = F.relu(self.lin0(data.x))                    #data.x size [batch_num_nodes, num_node_features]\n",
        "        for _ in range(3):\n",
        "            out = F.relu(self.conv(out, data.edge_index))\n",
        "        out = self.set2set(out, data.batch)                #[batch_num_nodes, dim] ==> [batch_num_graphs, dim*2]\n",
        "        out = F.relu(self.lin1(out))\n",
        "        out = self.lin2(out)\n",
        "        return out.view(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "eumyKXt1-ZOH"
      },
      "outputs": [],
      "source": [
        "# define training and evaluation functions\n",
        "def train(model, loader, optimizer, criterion, device):\n",
        "    \"\"\"Takes in training dataset loader,\n",
        "    train the model one step,\n",
        "    update the parameters,\n",
        "    return the current loss\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in loader:\n",
        "      data = data.to(device)\n",
        "      optimizer.zero_grad()       # Reset gradients\n",
        "      output = model(data)        # Forward pass\n",
        "      loss = criterion(output, data.y.to(device))  # Compute the loss\n",
        "      loss.backward()             # Backpropagate the gradients\n",
        "      optimizer.step()            # Update the weights\n",
        "      total_loss += loss.item() * data.num_graphs  # Update total loss\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "def eval(model, loader, criterion, device):\n",
        "    \"\"\"Takes the validation dataset loader,\n",
        "    return the validation MAE\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "          data = data.to(device)\n",
        "          output = model(data)\n",
        "          loss = criterion(output, data.y.to(device))\n",
        "          total_loss += loss.item() * data.num_graphs\n",
        "    return total_loss / len(loader.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "29AeJ7Q6-ZOI"
      },
      "outputs": [],
      "source": [
        "# load the datasets\n",
        "train_loader = DataLoader(train_data, batch_size=128)\n",
        "validate_loader = DataLoader(validate_data, batch_size=128)\n",
        "test_loader = DataLoader(test_data, batch_size=8)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Net(num_features=11, dim=64).to(device)\n",
        "optimizer = torch.optim.Adamax(model.parameters(), lr=0.005)\n",
        "criterion = torch.nn.L1Loss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uF38HtRBD89Q",
        "outputId": "d0639e3b-d891-4135-d03d-cb1232b8d166"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyAA9GLE-ZOJ",
        "outputId": "af2b45ae-48da-4a62-fe29-b20f7c2c6831"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 01, Train Loss: 1.1702, Val Loss: 1.0050\n",
            "Epoch: 02, Train Loss: 0.9545, Val Loss: 0.9197\n",
            "Epoch: 03, Train Loss: 0.8850, Val Loss: 0.8604\n",
            "Epoch: 04, Train Loss: 0.8207, Val Loss: 0.8096\n",
            "Epoch: 05, Train Loss: 0.7919, Val Loss: 0.7877\n",
            "Epoch: 06, Train Loss: 0.7854, Val Loss: 0.7787\n",
            "Epoch: 07, Train Loss: 0.7752, Val Loss: 0.7625\n",
            "Epoch: 08, Train Loss: 0.7683, Val Loss: 0.7698\n",
            "Epoch: 09, Train Loss: 0.7632, Val Loss: 0.7584\n",
            "Epoch: 10, Train Loss: 0.7560, Val Loss: 0.7491\n",
            "Epoch: 11, Train Loss: 0.7482, Val Loss: 0.7410\n",
            "Epoch: 12, Train Loss: 0.7411, Val Loss: 0.7466\n",
            "Epoch: 13, Train Loss: 0.7390, Val Loss: 0.7324\n",
            "Epoch: 14, Train Loss: 0.7327, Val Loss: 0.7347\n",
            "Epoch: 15, Train Loss: 0.7285, Val Loss: 0.7278\n",
            "Epoch: 16, Train Loss: 0.7272, Val Loss: 0.7254\n",
            "Epoch: 17, Train Loss: 0.7218, Val Loss: 0.7281\n",
            "Epoch: 18, Train Loss: 0.7167, Val Loss: 0.7249\n",
            "Epoch: 19, Train Loss: 0.7126, Val Loss: 0.7277\n",
            "Epoch: 20, Train Loss: 0.7096, Val Loss: 0.7288\n",
            "Epoch: 21, Train Loss: 0.7085, Val Loss: 0.7231\n",
            "Epoch: 22, Train Loss: 0.7004, Val Loss: 0.7165\n",
            "Epoch: 23, Train Loss: 0.6953, Val Loss: 0.7113\n",
            "Epoch: 24, Train Loss: 0.6903, Val Loss: 0.6998\n",
            "Epoch: 25, Train Loss: 0.6871, Val Loss: 0.6945\n",
            "Epoch: 26, Train Loss: 0.6870, Val Loss: 0.6978\n",
            "Epoch: 27, Train Loss: 0.6791, Val Loss: 0.6854\n",
            "Epoch: 28, Train Loss: 0.6767, Val Loss: 0.6868\n",
            "Epoch: 29, Train Loss: 0.6813, Val Loss: 0.6877\n",
            "Epoch: 30, Train Loss: 0.6732, Val Loss: 0.6866\n",
            "Epoch: 31, Train Loss: 0.6715, Val Loss: 0.6766\n",
            "Epoch: 32, Train Loss: 0.6666, Val Loss: 0.6781\n",
            "Epoch: 33, Train Loss: 0.6649, Val Loss: 0.6803\n",
            "Epoch: 34, Train Loss: 0.6574, Val Loss: 0.6765\n",
            "Epoch: 35, Train Loss: 0.6565, Val Loss: 0.6690\n",
            "Epoch: 36, Train Loss: 0.6574, Val Loss: 0.6690\n",
            "Epoch: 37, Train Loss: 0.6504, Val Loss: 0.6625\n",
            "Epoch: 38, Train Loss: 0.6481, Val Loss: 0.6589\n",
            "Epoch: 39, Train Loss: 0.6438, Val Loss: 0.6659\n",
            "Epoch: 40, Train Loss: 0.6403, Val Loss: 0.6656\n",
            "Epoch: 41, Train Loss: 0.6381, Val Loss: 0.6636\n",
            "Epoch: 42, Train Loss: 0.6342, Val Loss: 0.6492\n",
            "Epoch: 43, Train Loss: 0.6312, Val Loss: 0.6393\n",
            "Epoch: 44, Train Loss: 0.6309, Val Loss: 0.6482\n",
            "Epoch: 45, Train Loss: 0.6260, Val Loss: 0.6436\n",
            "Epoch: 46, Train Loss: 0.6233, Val Loss: 0.6426\n",
            "Epoch: 47, Train Loss: 0.6248, Val Loss: 0.6448\n",
            "Epoch: 48, Train Loss: 0.6187, Val Loss: 0.6373\n",
            "Epoch: 49, Train Loss: 0.6156, Val Loss: 0.6391\n",
            "Epoch: 50, Train Loss: 0.6199, Val Loss: 0.6419\n",
            "Epoch: 51, Train Loss: 0.6155, Val Loss: 0.6377\n",
            "Epoch: 52, Train Loss: 0.6136, Val Loss: 0.6361\n",
            "Epoch: 53, Train Loss: 0.6090, Val Loss: 0.6340\n",
            "Epoch: 54, Train Loss: 0.6082, Val Loss: 0.6306\n",
            "Epoch: 55, Train Loss: 0.6089, Val Loss: 0.6482\n",
            "Epoch: 56, Train Loss: 0.6040, Val Loss: 0.6188\n",
            "Epoch: 57, Train Loss: 0.6041, Val Loss: 0.6290\n",
            "Epoch: 58, Train Loss: 0.5979, Val Loss: 0.6257\n",
            "Epoch: 59, Train Loss: 0.5965, Val Loss: 0.6308\n",
            "Epoch: 60, Train Loss: 0.5988, Val Loss: 0.6296\n",
            "Epoch: 61, Train Loss: 0.5942, Val Loss: 0.6226\n",
            "Epoch: 62, Train Loss: 0.5907, Val Loss: 0.6351\n",
            "Epoch: 63, Train Loss: 0.5964, Val Loss: 0.6331\n",
            "Epoch: 64, Train Loss: 0.5882, Val Loss: 0.6322\n",
            "Epoch: 65, Train Loss: 0.5831, Val Loss: 0.6301\n",
            "Epoch: 66, Train Loss: 0.5837, Val Loss: 0.6332\n",
            "Epoch: 67, Train Loss: 0.5831, Val Loss: 0.6241\n",
            "Epoch: 68, Train Loss: 0.5798, Val Loss: 0.6159\n",
            "Epoch: 69, Train Loss: 0.5757, Val Loss: 0.6295\n",
            "Epoch: 70, Train Loss: 0.5756, Val Loss: 0.6422\n",
            "Epoch: 71, Train Loss: 0.5772, Val Loss: 0.6290\n",
            "Epoch: 72, Train Loss: 0.5726, Val Loss: 0.6319\n",
            "Epoch: 73, Train Loss: 0.5682, Val Loss: 0.6301\n",
            "Epoch: 74, Train Loss: 0.5652, Val Loss: 0.6482\n",
            "Epoch: 75, Train Loss: 0.5617, Val Loss: 0.6537\n",
            "Epoch: 76, Train Loss: 0.5621, Val Loss: 0.6325\n",
            "Epoch: 77, Train Loss: 0.5620, Val Loss: 0.6265\n",
            "Epoch: 78, Train Loss: 0.5586, Val Loss: 0.6244\n",
            "Epoch: 79, Train Loss: 0.5576, Val Loss: 0.6243\n",
            "Epoch: 80, Train Loss: 0.5578, Val Loss: 0.6273\n",
            "Epoch: 81, Train Loss: 0.5537, Val Loss: 0.6284\n",
            "Epoch: 82, Train Loss: 0.5571, Val Loss: 0.6180\n",
            "Epoch: 83, Train Loss: 0.5499, Val Loss: 0.6062\n",
            "Epoch: 84, Train Loss: 0.5502, Val Loss: 0.6240\n",
            "Epoch: 85, Train Loss: 0.5491, Val Loss: 0.6196\n",
            "Epoch: 86, Train Loss: 0.5496, Val Loss: 0.6122\n",
            "Epoch: 87, Train Loss: 0.5467, Val Loss: 0.6169\n",
            "Epoch: 88, Train Loss: 0.5521, Val Loss: 0.6082\n",
            "Epoch: 89, Train Loss: 0.5471, Val Loss: 0.6065\n",
            "Epoch: 90, Train Loss: 0.5439, Val Loss: 0.6064\n",
            "Epoch: 91, Train Loss: 0.5433, Val Loss: 0.6276\n",
            "Epoch: 92, Train Loss: 0.5455, Val Loss: 0.6259\n",
            "Epoch: 93, Train Loss: 0.5415, Val Loss: 0.6281\n",
            "Epoch: 94, Train Loss: 0.5437, Val Loss: 0.6151\n",
            "Epoch: 95, Train Loss: 0.5432, Val Loss: 0.6298\n",
            "Epoch: 96, Train Loss: 0.5370, Val Loss: 0.6219\n",
            "Epoch: 97, Train Loss: 0.5340, Val Loss: 0.6262\n",
            "Epoch: 98, Train Loss: 0.5299, Val Loss: 0.6323\n",
            "Epoch: 99, Train Loss: 0.5323, Val Loss: 0.6532\n",
            "Epoch: 100, Train Loss: 0.5329, Val Loss: 0.6484\n",
            "Epoch: 101, Train Loss: 0.5272, Val Loss: 0.6306\n",
            "Epoch: 102, Train Loss: 0.5263, Val Loss: 0.6320\n",
            "Epoch: 103, Train Loss: 0.5235, Val Loss: 0.6260\n",
            "Epoch: 104, Train Loss: 0.5238, Val Loss: 0.6296\n",
            "Epoch: 105, Train Loss: 0.5245, Val Loss: 0.6301\n",
            "Epoch: 106, Train Loss: 0.5212, Val Loss: 0.6448\n",
            "Epoch: 107, Train Loss: 0.5212, Val Loss: 0.6184\n",
            "Epoch: 108, Train Loss: 0.5147, Val Loss: 0.6316\n",
            "Epoch: 109, Train Loss: 0.5185, Val Loss: 0.6212\n",
            "Epoch: 110, Train Loss: 0.5204, Val Loss: 0.6176\n",
            "Epoch: 111, Train Loss: 0.5131, Val Loss: 0.6471\n",
            "Epoch: 112, Train Loss: 0.5154, Val Loss: 0.6266\n",
            "Epoch: 113, Train Loss: 0.5139, Val Loss: 0.6093\n",
            "Epoch: 114, Train Loss: 0.5159, Val Loss: 0.6129\n",
            "Epoch: 115, Train Loss: 0.5173, Val Loss: 0.6250\n",
            "Epoch: 116, Train Loss: 0.5163, Val Loss: 0.6212\n",
            "Epoch: 117, Train Loss: 0.5137, Val Loss: 0.6257\n",
            "Epoch: 118, Train Loss: 0.5120, Val Loss: 0.6254\n",
            "Epoch: 119, Train Loss: 0.5090, Val Loss: 0.6230\n",
            "Epoch: 120, Train Loss: 0.5084, Val Loss: 0.6215\n",
            "Epoch: 121, Train Loss: 0.5142, Val Loss: 0.6338\n",
            "Epoch: 122, Train Loss: 0.5066, Val Loss: 0.6174\n",
            "Epoch: 123, Train Loss: 0.5083, Val Loss: 0.6295\n",
            "Epoch: 124, Train Loss: 0.5092, Val Loss: 0.6541\n",
            "Epoch: 125, Train Loss: 0.5047, Val Loss: 0.6437\n",
            "Epoch: 126, Train Loss: 0.5014, Val Loss: 0.6189\n",
            "Epoch: 127, Train Loss: 0.5015, Val Loss: 0.6363\n",
            "Epoch: 128, Train Loss: 0.4993, Val Loss: 0.6137\n",
            "Epoch: 129, Train Loss: 0.4976, Val Loss: 0.6186\n",
            "Epoch: 130, Train Loss: 0.4972, Val Loss: 0.6236\n",
            "Epoch: 131, Train Loss: 0.4975, Val Loss: 0.6101\n",
            "Epoch: 132, Train Loss: 0.4925, Val Loss: 0.6151\n",
            "Epoch: 133, Train Loss: 0.4914, Val Loss: 0.6216\n",
            "Epoch: 134, Train Loss: 0.4940, Val Loss: 0.6123\n",
            "Epoch: 135, Train Loss: 0.4880, Val Loss: 0.6083\n",
            "Epoch: 136, Train Loss: 0.4844, Val Loss: 0.6174\n",
            "Epoch: 137, Train Loss: 0.4882, Val Loss: 0.6115\n",
            "Epoch: 138, Train Loss: 0.4865, Val Loss: 0.6076\n",
            "Epoch: 139, Train Loss: 0.4863, Val Loss: 0.6216\n",
            "Epoch: 140, Train Loss: 0.4862, Val Loss: 0.6152\n",
            "Epoch: 141, Train Loss: 0.4865, Val Loss: 0.6145\n",
            "Epoch: 142, Train Loss: 0.4875, Val Loss: 0.6009\n",
            "Epoch: 143, Train Loss: 0.4833, Val Loss: 0.6035\n",
            "Epoch: 144, Train Loss: 0.4791, Val Loss: 0.6123\n",
            "Epoch: 145, Train Loss: 0.4868, Val Loss: 0.6070\n",
            "Epoch: 146, Train Loss: 0.4768, Val Loss: 0.6150\n",
            "Epoch: 147, Train Loss: 0.4829, Val Loss: 0.6121\n",
            "Epoch: 148, Train Loss: 0.4772, Val Loss: 0.6168\n",
            "Epoch: 149, Train Loss: 0.4775, Val Loss: 0.6144\n",
            "Epoch: 150, Train Loss: 0.4772, Val Loss: 0.6209\n",
            "Epoch: 151, Train Loss: 0.4778, Val Loss: 0.6218\n",
            "Epoch: 152, Train Loss: 0.4720, Val Loss: 0.6201\n",
            "Epoch: 153, Train Loss: 0.4750, Val Loss: 0.6255\n",
            "Epoch: 154, Train Loss: 0.4747, Val Loss: 0.5993\n",
            "Epoch: 155, Train Loss: 0.4765, Val Loss: 0.6310\n",
            "Epoch: 156, Train Loss: 0.4770, Val Loss: 0.6326\n",
            "Epoch: 157, Train Loss: 0.4778, Val Loss: 0.6354\n",
            "Epoch: 158, Train Loss: 0.4771, Val Loss: 0.6312\n",
            "Epoch: 159, Train Loss: 0.4716, Val Loss: 0.6136\n",
            "Epoch: 160, Train Loss: 0.4704, Val Loss: 0.6210\n",
            "Epoch: 161, Train Loss: 0.4743, Val Loss: 0.6278\n",
            "Epoch: 162, Train Loss: 0.4707, Val Loss: 0.6240\n",
            "Epoch: 163, Train Loss: 0.4712, Val Loss: 0.6337\n",
            "Epoch: 164, Train Loss: 0.4714, Val Loss: 0.6204\n",
            "Epoch: 165, Train Loss: 0.4743, Val Loss: 0.6226\n",
            "Epoch: 166, Train Loss: 0.4671, Val Loss: 0.6247\n",
            "Epoch: 167, Train Loss: 0.4655, Val Loss: 0.6139\n",
            "Epoch: 168, Train Loss: 0.4653, Val Loss: 0.6129\n",
            "Epoch: 169, Train Loss: 0.4649, Val Loss: 0.6198\n",
            "Epoch: 170, Train Loss: 0.4616, Val Loss: 0.6129\n",
            "Epoch: 171, Train Loss: 0.4613, Val Loss: 0.6061\n",
            "Epoch: 172, Train Loss: 0.4622, Val Loss: 0.6188\n",
            "Epoch: 173, Train Loss: 0.4626, Val Loss: 0.6116\n",
            "Epoch: 174, Train Loss: 0.4631, Val Loss: 0.6223\n",
            "Epoch: 175, Train Loss: 0.4647, Val Loss: 0.6081\n",
            "Epoch: 176, Train Loss: 0.4658, Val Loss: 0.6120\n",
            "Epoch: 177, Train Loss: 0.4605, Val Loss: 0.6079\n",
            "Epoch: 178, Train Loss: 0.4582, Val Loss: 0.6128\n",
            "Epoch: 179, Train Loss: 0.4604, Val Loss: 0.6113\n",
            "Epoch: 180, Train Loss: 0.4591, Val Loss: 0.6103\n",
            "Epoch: 181, Train Loss: 0.4540, Val Loss: 0.6206\n",
            "Epoch: 182, Train Loss: 0.4541, Val Loss: 0.6166\n",
            "Epoch: 183, Train Loss: 0.4592, Val Loss: 0.6259\n",
            "Epoch: 184, Train Loss: 0.4607, Val Loss: 0.6129\n",
            "Epoch: 185, Train Loss: 0.4547, Val Loss: 0.6286\n",
            "Epoch: 186, Train Loss: 0.4606, Val Loss: 0.6103\n",
            "Epoch: 187, Train Loss: 0.4576, Val Loss: 0.6231\n",
            "Epoch: 188, Train Loss: 0.4521, Val Loss: 0.6197\n",
            "Epoch: 189, Train Loss: 0.4524, Val Loss: 0.6193\n",
            "Epoch: 190, Train Loss: 0.4512, Val Loss: 0.6126\n",
            "Epoch: 191, Train Loss: 0.4495, Val Loss: 0.6240\n",
            "Epoch: 192, Train Loss: 0.4500, Val Loss: 0.6059\n",
            "Epoch: 193, Train Loss: 0.4468, Val Loss: 0.6117\n",
            "Epoch: 194, Train Loss: 0.4498, Val Loss: 0.6195\n",
            "Epoch: 195, Train Loss: 0.4463, Val Loss: 0.6097\n",
            "Epoch: 196, Train Loss: 0.4420, Val Loss: 0.6122\n",
            "Epoch: 197, Train Loss: 0.4455, Val Loss: 0.6040\n",
            "Epoch: 198, Train Loss: 0.4434, Val Loss: 0.6148\n",
            "Epoch: 199, Train Loss: 0.4457, Val Loss: 0.6126\n"
          ]
        }
      ],
      "source": [
        "# training\n",
        "num_epochs = 200\n",
        "for epoch in range(1, num_epochs):\n",
        "    \"\"\"Calculate loss and\n",
        "    validation MAE\"\"\"\n",
        "    train_loss = train(model, train_loader, optimizer, criterion, device)\n",
        "    val_loss = eval(model, validate_loader, criterion, device)\n",
        "    print(f\"Epoch: {epoch:02d}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ykihqvj1-ZOK"
      },
      "outputs": [],
      "source": [
        "# predict\n",
        "y_pred = []\n",
        "Idx = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "      output = model(data.to(device))\n",
        "      y_pred.extend(output.cpu().numpy())\n",
        "      Idx.extend(data.name)\n",
        "\n",
        "assert(len(Idx) == len(y_pred))\n",
        "df = pd.DataFrame({\"Idx\": Idx, \"labels\": y_pred})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Ib5a1tFn-ZOK"
      },
      "outputs": [],
      "source": [
        "# upload solution\n",
        "df.columns = ['Idx', 'labels']\n",
        "df.to_csv(\"template\", index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "interpreter": {
      "hash": "61b4062b24dfb1010f420dad5aa3bd73a4d2af47d0ec44eafec465a35a9d7239"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
